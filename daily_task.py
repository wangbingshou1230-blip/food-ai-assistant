import os
import requests
import pandas as pd
import json
from datetime import datetime
from bs4 import BeautifulSoup
import time

# ================= âš™ï¸ é…ç½®åŒº =================
# ä½ å…³å¿ƒçš„å…³é”®è¯ï¼Œå¯ä»¥æ˜¯ "é¢„åˆ¶èœ", "ä¹³æ¶²å‡èƒ¶", "é£Ÿå“å®‰å…¨"
KEYWORDS = ["é£Ÿå“å®‰å…¨", "é¢„åˆ¶èœ"] 
OUTPUT_FOLDER = "output_files"

# ================= ğŸ” å¯†é’¥è·å– (è‡ªåŠ¨é€‚é… GitHub Secrets) =================
def get_env_var(key_name):
    # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è·å– (GitHub Actions è¿è¡Œæ—¶)
    val = os.environ.get(key_name)
    if not val:
        # æœ¬åœ°æµ‹è¯•æ—¶ï¼Œå°è¯•è¯» config.json
        try:
            if os.path.exists("config.json"):
                with open("config.json", "r", encoding="utf-8") as f:
                    return json.load(f).get(key_name)
        except:
            pass
    return val

# ================= ğŸ•·ï¸ 1. çˆ¬è™«æ¨¡å— =================
def run_crawler():
    print("ğŸ•·ï¸ ä¾¦å¯Ÿå…µå¯åŠ¨ï¼Œå¼€å§‹æŠ“å–æ–°é—»...")
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    all_news = []
    
    for kw in KEYWORDS:
        # ç™¾åº¦èµ„è®¯æœç´¢é“¾æ¥
        url = f"https://www.baidu.com/s?tn=news&rtt=1&bsst=1&cl=2&wd={kw}"
        try:
            res = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–°é—»æ ‡é¢˜ (ç™¾åº¦æ–°é—»ç»“æ„å¯èƒ½ä¼šå˜ï¼Œè¿™é‡Œç”¨è¾ƒé€šç”¨çš„æ‰¾æ³•)
            # å¯»æ‰¾æ‰€æœ‰ h3 æ ‡ç­¾ï¼Œä¸” class åŒ…å« news-title
            items = soup.find_all('h3', class_=lambda x: x and 'news-title' in x)
            
            for item in items:
                link = item.find('a')
                if link:
                    title = link.get_text().strip()
                    href = link['href']
                    source_span = item.parent.find('span', class_=lambda x: x and 'c-color-gray' in x)
                    source = source_span.get_text().strip() if source_span else "æœªçŸ¥æ¥æº"
                    
                    all_news.append({
                        "å…³é”®è¯": kw,
                        "æ ‡é¢˜": title,
                        "æ¥æº": source,
                        "é“¾æ¥": href,
                        "çˆ¬å–æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M")
                    })
            print(f"âœ… å…³é”®è¯ã€{kw}ã€‘æŠ“å–å®Œæˆ")
            time.sleep(1) # ä¼‘æ¯ä¸€ä¸‹ï¼Œé˜²æ­¢è¢«å°
        except Exception as e:
            print(f"âŒ å…³é”®è¯ã€{kw}ã€‘æŠ“å–å¤±è´¥: {e}")

    if not all_news:
        print("âš ï¸ æœªæŠ“å–åˆ°ä»»ä½•æ•°æ®")
        return None

    # ä¿å­˜ä¸º Excel
    if not os.path.exists(OUTPUT_FOLDER):
        os.makedirs(OUTPUT_FOLDER)
    
    # æ–‡ä»¶åå¸¦æ—¥æœŸï¼š2026-01-18_DailyNews.xlsx
    today = datetime.now().strftime("%Y-%m-%d")
    filename = f"{OUTPUT_FOLDER}/{today}_DailyNews.xlsx"
    
    df = pd.DataFrame(all_news)
    df.to_excel(filename, index=False)
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜: {filename}")
    return df

# ================= ğŸ§  2. AI åˆ†ææ¨¡å— =================
def analyze_news(df):
    print("ğŸ§  AI æ­£åœ¨é˜…è¯»æ–°é—»...")
    api_key = get_env_var("deepseek_api_key")
    if not api_key:
        print("âŒ ç¼ºå°‘ DeepSeek Keyï¼Œè·³è¿‡åˆ†æ")
        return "ä»Šæ—¥çˆ¬å–å®Œæˆï¼Œä½†æœªé…ç½® AI Keyï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦ã€‚"

    # å–å‰ 20 æ¡æ ‡é¢˜
    titles = df["æ ‡é¢˜"].head(20).tolist()
    text_block = "\n".join(titles)

    # æ„é€  Prompt
    prompt = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªé£Ÿå“å®‰å…¨æƒ…æŠ¥å®˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ–°é—»æ ‡é¢˜ï¼Œç®€è¦æ€»ç»“ä»Šæ—¥èˆ†æƒ…çƒ­ç‚¹ (100å­—ä»¥å†…)ã€‚å¦‚æœæ— ç‰¹æ®Šå¤§äº‹ï¼Œè¯·ç®€æŠ¥å¹³å®‰ã€‚"},
        {"role": "user", "content": f"ä»Šæ—¥æ–°é—»æ ‡é¢˜ï¼š\n{text_block}"}
    ]

    try:
        res = requests.post(
            "https://api.deepseek.com/chat/completions",
            headers={"Authorization": f"Bearer {api_key}"},
            json={"model": "deepseek-chat", "messages": prompt, "stream": False},
            timeout=30
        )
        if res.status_code == 200:
            return res.json()['choices'][0]['message']['content']
        else:
            return f"AI æŠ¥é”™: {res.text}"
    except Exception as e:
        return f"è¯·æ±‚å¤±è´¥: {e}"

# ================= ğŸ“¡ 3. Bark æ¨é€æ¨¡å— =================
def push_bark(summary):
    print("ğŸ“¡ å‡†å¤‡æ¨é€...")
    device_key = get_env_var("bark_device_key")
    server = get_env_var("bark_server")
    if not server: server = "https://api.day.app"
    
    if not device_key:
        print("âš ï¸ æœªé…ç½® Bark Keyï¼Œè·³è¿‡æ¨é€")
        return

    title = "FoodAIæ—©æŠ¥"
    content = summary[:200] # æˆªå–é˜²æ­¢è¶…é•¿
    
    url = f"{server.rstrip('/')}/{device_key}/{title}/{content}"
    try:
        requests.get(url, timeout=10)
        print("âœ… æ¨é€æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨é€å‡ºé”™: {e}")

# ================= ğŸš€ ä¸»æµç¨‹ =================
if __name__ == "__main__":
    df = run_crawler()
    if df is not None:
        summary = analyze_news(df)
        print(f"ğŸ“Š æ‘˜è¦: {summary}")
        push_bark(summary)
    else:
        print("ğŸ’¤ ä»Šæ—¥æ— æ–°æ•°æ®")