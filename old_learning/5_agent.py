import requests
from bs4 import BeautifulSoup
import csv
import datetime
import json
import os

# ==================== é…ç½®ä¸­å¿ƒ ====================
# 1. ä½ çš„ DeepSeek å¯†é’¥
API_KEY = "sk-44104f41c16f42748973c225aff64f0f"  # <--- ã€è¯·åŠ¡å¿…æ›¿æ¢ä¸ºä½ åˆšæ‰å……å€¼è¿‡çš„å¯†é’¥ã€‘
# 2. ä½ æƒ³ç›‘æ§çš„å…³é”®è¯
SEARCH_KEYWORD = "é£Ÿå“è¡Œä¸š AI åº”ç”¨"  # <--- å¯ä»¥éšæ—¶æ”¹è¿™ä¸ªè¯ï¼Œæ¯”å¦‚â€œé£Ÿå“è¡Œä¸š AI è½¬å‹â€
# ================================================

def step1_crawl_data():
    """ç¬¬ä¸€æ­¥ï¼šå»å¿…åº”æŠ“å–æ•°æ®"""
    print(f"\nğŸš€ å¯åŠ¨é˜¶æ®µä¸€ï¼šæ­£åœ¨å…¨ç½‘æœç´¢ '{SEARCH_KEYWORD}' ...")
    
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36"}
    url = f"https://cn.bing.com/search?q={SEARCH_KEYWORD}"
    
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        results = soup.find_all('h2')
        
        data = []
        for tag in results:
            link = tag.find('a')
            if link:
                title = link.get_text()
                href = link.get('href')
                data.append(f"æ ‡é¢˜ï¼š{title} | é“¾æ¥ï¼š{href}")
        
        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        if data:
            filename = "latest_data.txt"
            with open(filename, "w", encoding="utf-8") as f:
                f.write("\n".join(data))
            print(f"âœ… æŠ“å–æˆåŠŸï¼šæ‰¾åˆ° {len(data)} æ¡ä¿¡æ¯ï¼Œå·²ç¼“å­˜ã€‚")
            return filename
        else:
            print("âŒ æŠ“å–å¤±è´¥ï¼šæœªæ‰¾åˆ°å†…å®¹ï¼ˆå¯èƒ½æ˜¯åçˆ¬è™«æ‹¦æˆªï¼‰ã€‚")
            return None
            
    except Exception as e:
        print(f"âŒ æŠ“å–é˜¶æ®µå‡ºé”™ï¼š{e}")
        return None

def step2_analyze_data(filename):
    """ç¬¬äºŒæ­¥ï¼šè°ƒç”¨ DeepSeek åˆ†æ"""
    print(f"\nğŸ§  å¯åŠ¨é˜¶æ®µäºŒï¼šæ­£åœ¨è°ƒç”¨ DeepSeek å¤§è„‘è¿›è¡Œåˆ†æ ...")
    
    with open(filename, "r", encoding="utf-8") as f:
        content = f.read()
    
    url = "https://api.deepseek.com/chat/completions"
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {API_KEY}"}
    
    # æŒ‡ä»¤ï¼šè®© AI æ‰®æ¼”æ¯’èˆŒçš„èŒä¸šé¡¾é—®
    prompt = """
    ä½ æ˜¯ä¸€ä¸ªçŠ€åˆ©çš„èŒä¸šè§„åˆ’é¡¾é—®ã€‚ç”¨æˆ·æƒ³è½¬è¡Œåš AIã€‚
    è¯·åˆ†æä¸‹é¢æŠ“å–åˆ°çš„æœç´¢ç»“æœï¼š
    1. æŒ‘å‡ºæœ€æœ‰ä»·å€¼çš„ 2 æ¡ä¿¡æ¯ï¼ˆæ‹›è˜æˆ–æ–‡ç« ï¼‰ã€‚
    2. åªè¦å¹²è´§ï¼Œä¸è¦åºŸè¯ã€‚
    3. å¦‚æœçœ‹èµ·æ¥éƒ½æ˜¯åƒåœ¾å¹¿å‘Šï¼Œè¯·ç›´æ¥åæ§½ã€‚
    """
    
    payload = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "system", "content": prompt},
            {"role": "user", "content": content}
        ]
    }
    
    try:
        resp = requests.post(url, headers=headers, data=json.dumps(payload))
        if resp.status_code == 200:
            return resp.json()['choices'][0]['message']['content']
        else:
            return f"API è°ƒç”¨å¤±è´¥ï¼š{resp.text}"
    except Exception as e:
        return f"åˆ†æé˜¶æ®µå‡ºé”™ï¼š{e}"

def main():
    print("==="*10 + " è‡ªåŠ¨åŒ– Agent å¯åŠ¨ " + "==="*10)
    
    # é¡ºåºæ‰§è¡Œ
    data_file = step1_crawl_data()
    
    if data_file:
        report = step2_analyze_data(data_file)
        
        print("\n" + "==="*10 + " æœ€ç»ˆåˆ†ææŠ¥å‘Š " + "==="*10)
        print(report)
        print("==="*30)
        
        # è¿™é‡Œä»¥åå¯ä»¥åŠ ï¼šå‘é€é‚®ä»¶ç»™æˆ‘çš„åŠŸèƒ½
    
    print("\nâœ… ä»»åŠ¡ç»“æŸã€‚")

if __name__ == "__main__":
    main()